{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c34150",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:22.764724Z",
     "iopub.status.busy": "2025-01-23T11:29:22.764473Z",
     "iopub.status.idle": "2025-01-23T11:29:32.601059Z",
     "shell.execute_reply": "2025-01-23T11:29:32.600246Z"
    },
    "papermill": {
     "duration": 9.843964,
     "end_time": "2025-01-23T11:29:32.602369",
     "exception": false,
     "start_time": "2025-01-23T11:29:22.758405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c96392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.612863Z",
     "iopub.status.busy": "2025-01-23T11:29:32.612509Z",
     "iopub.status.idle": "2025-01-23T11:29:32.621817Z",
     "shell.execute_reply": "2025-01-23T11:29:32.621180Z"
    },
    "papermill": {
     "duration": 0.015741,
     "end_time": "2025-01-23T11:29:32.623110",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.607369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_threshold,\n",
    "        vocab_file=\"/kaggle/input/vocab/pytorch/default/1/vocab.pkl\",  \n",
    "        start_word=\"<start>\",\n",
    "        end_word=\"<end>\",\n",
    "        unk_word=\"<unk>\",\n",
    "        annotations_file=\"/kaggle/input/coco-image-caption/annotations_trainval2014/annotations/captions_train2014.json\",  # COCO 2014 training annotations\n",
    "        vocab_from_file=False,\n",
    "    ):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "          vocab_threshold: Minimum word count threshold.\n",
    "          vocab_file: File containing the vocabulary.\n",
    "          start_word: Special word denoting sentence start.\n",
    "          end_word: Special word denoting sentence end.\n",
    "          unk_word: Special word denoting unknown words.\n",
    "          annotations_file: Path for train annotation file.\n",
    "          vocab_from_file: If False, create vocab from scratch and override any existing vocab_file\n",
    "                           If True, load vocab from existing vocab_file, if it exists\n",
    "        \"\"\"\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Load the vocabulary from file OR build the vocabulary from scratch.\"\"\"\n",
    "        if os.path.exists(self.vocab_file) and self.vocab_from_file:\n",
    "            with open(self.vocab_file, \"rb\") as f:\n",
    "                vocab = pickle.load(f)\n",
    "            self.word2idx = vocab.word2idx\n",
    "            self.idx2word = vocab.idx2word\n",
    "            print(\"Vocabulary successfully loaded from vocab.pkl file!\")\n",
    "\n",
    "        # create a new vocab file\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, \"wb\") as f:\n",
    "                pickle.dump(self, f)\n",
    "\n",
    "    def build_vocab(self):\n",
    "        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
    "        self.init_vocab()\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a token to the vocabulary.\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, idx in enumerate(ids):\n",
    "            caption = str(coco.anns[idx][\"caption\"])\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "\n",
    "        # keep only words that repeated more than threshold times in the final vocabulary\n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10813f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.633130Z",
     "iopub.status.busy": "2025-01-23T11:29:32.632885Z",
     "iopub.status.idle": "2025-01-23T11:29:32.637516Z",
     "shell.execute_reply": "2025-01-23T11:29:32.636908Z"
    },
    "papermill": {
     "duration": 0.011106,
     "end_time": "2025-01-23T11:29:32.638654",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.627548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "def clean_sentence(output, idx2word):\n",
    "    sentence = \"\"\n",
    "    for i in output:\n",
    "        word = idx2word[i]\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == 1:\n",
    "            break\n",
    "        if i == 18:\n",
    "            sentence = sentence + word\n",
    "        else:\n",
    "            sentence = sentence + \" \" + word\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def bleu_score(true_sentences, predicted_sentences):\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    for img_id in set(true_sentences.keys()).intersection(\n",
    "        set(predicted_sentences.keys())\n",
    "    ):\n",
    "        img_refs = [cap.split() for cap in true_sentences[img_id]]\n",
    "        references.append(img_refs)\n",
    "        hypotheses.append(predicted_sentences[img_id][0].strip().split())\n",
    "\n",
    "    return corpus_bleu(references, hypotheses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df67368f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.648385Z",
     "iopub.status.busy": "2025-01-23T11:29:32.648192Z",
     "iopub.status.idle": "2025-01-23T11:29:32.658770Z",
     "shell.execute_reply": "2025-01-23T11:29:32.658006Z"
    },
    "papermill": {
     "duration": 0.016881,
     "end_time": "2025-01-23T11:29:32.660101",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.643220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CoCoDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform,\n",
    "        mode,\n",
    "        batch_size,\n",
    "        vocab_threshold,\n",
    "        vocab_file,\n",
    "        start_word=\"<start>\",\n",
    "        end_word=\"<end>\",\n",
    "        unk_word=\"<unk>\",\n",
    "        annotations_file=None,\n",
    "        vocab_from_file=False,\n",
    "        img_folder=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the COCO dataset.\n",
    "\n",
    "        Args:\n",
    "            transform: Image transformations (e.g., resizing, normalization).\n",
    "            mode: One of \"train\" or \"test\".\n",
    "            batch_size: Batch size for training.\n",
    "            vocab_threshold: Minimum word frequency threshold for vocabulary.\n",
    "            vocab_file: Path to save/load the vocabulary.\n",
    "            start_word: Special token for the start of a sentence.\n",
    "            end_word: Special token for the end of a sentence.\n",
    "            unk_word: Special token for unknown words.\n",
    "            annotations_file: Path to the COCO annotations file.\n",
    "            vocab_from_file: If True, load vocabulary from file; else, build from scratch.\n",
    "            img_folder: Path to the folder containing images.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.img_folder = img_folder\n",
    "\n",
    "        # Initialize vocabulary\n",
    "        self.vocab = Vocabulary(\n",
    "            vocab_threshold,\n",
    "            vocab_file,\n",
    "            start_word,\n",
    "            end_word,\n",
    "            unk_word,\n",
    "            annotations_file,\n",
    "            vocab_from_file,\n",
    "        )\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            # Load COCO annotations for training\n",
    "            self.coco = COCO(annotations_file)\n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print(\"Obtaining caption lengths...\")\n",
    "\n",
    "            # Tokenize captions and store their lengths\n",
    "            tokenized_captions = [\n",
    "                nltk.tokenize.word_tokenize(\n",
    "                    str(self.coco.anns[self.ids[index]][\"caption\"]).lower()\n",
    "                )\n",
    "                for index in tqdm(np.arange(len(self.ids)))\n",
    "            ]\n",
    "            self.caption_lengths = [len(token) for token in tokenized_captions]\n",
    "        else:\n",
    "            # Load COCO annotations for testing\n",
    "            test_info = json.loads(open(annotations_file).read())\n",
    "            self.paths = [item[\"file_name\"] for item in test_info[\"images\"]]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index: Index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            In training mode: (image, caption), where caption is a tensor of word indices.\n",
    "            In testing mode: (original_image, transformed_image).\n",
    "        \"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            # Get caption and image ID\n",
    "            ann_id = self.ids[index]\n",
    "            caption = self.coco.anns[ann_id][\"caption\"]\n",
    "            img_id = self.coco.anns[ann_id][\"image_id\"]\n",
    "            path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "\n",
    "            # Load and preprocess the image\n",
    "            image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word indices\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = [self.vocab(self.vocab.start_word)]  # Add <start> token\n",
    "            caption.extend([self.vocab(token) for token in tokens])  # Add word tokens\n",
    "            caption.append(self.vocab(self.vocab.end_word))  # Add <end> token\n",
    "            caption = torch.Tensor(caption).long()  # Convert to tensor\n",
    "\n",
    "            return image, caption\n",
    "\n",
    "        else:\n",
    "            # Get image path\n",
    "            path = self.paths[index]\n",
    "\n",
    "            # Load and preprocess the image\n",
    "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            orig_image = np.array(pil_image)  # Keep original image for visualization\n",
    "            image = self.transform(pil_image)  # Apply transformations\n",
    "\n",
    "            return orig_image, image\n",
    "\n",
    "    def get_train_indices(self):\n",
    "        \"\"\"\n",
    "        Get a batch of indices for training, ensuring all captions have the same length.\n",
    "\n",
    "        Returns:\n",
    "            List of indices.\n",
    "        \"\"\"\n",
    "        sel_length = np.random.choice(self.caption_lengths)  # Randomly select a caption length\n",
    "        all_indices = np.where(\n",
    "            [\n",
    "                self.caption_lengths[i] == sel_length\n",
    "                for i in np.arange(len(self.caption_lengths))\n",
    "            ]\n",
    "        )[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))  # Select batch_size indices\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            Number of samples.\n",
    "        \"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dbb17bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.669819Z",
     "iopub.status.busy": "2025-01-23T11:29:32.669619Z",
     "iopub.status.idle": "2025-01-23T11:29:32.675582Z",
     "shell.execute_reply": "2025-01-23T11:29:32.674934Z"
    },
    "papermill": {
     "duration": 0.012138,
     "end_time": "2025-01-23T11:29:32.676699",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.664561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "    transform,\n",
    "    mode=\"train\",\n",
    "    batch_size=32,\n",
    "    vocab_threshold=None,\n",
    "    vocab_file=\"/kaggle/input/vocab/pytorch/default/1/vocab.pkl\",\n",
    "    start_word=\"<start>\",\n",
    "    end_word=\"<end>\",\n",
    "    unk_word=\"<unk>\",\n",
    "    vocab_from_file=True,\n",
    "    num_workers=4,\n",
    "    img_folder=None,\n",
    "    annotations_file=None,\n",
    "):\n",
    "\n",
    "    assert mode in [\"train\", \"val\", \"test\"], \"mode must be one of 'train', 'val', or 'test'.\"\n",
    "\n",
    "    if mode == \"train\" and (not img_folder or not annotations_file):\n",
    "        raise ValueError(\"img_folder and annotations_file must be provided in 'train' mode.\")\n",
    "\n",
    "    # Initialize COCO dataset\n",
    "    dataset = CoCoDataset(\n",
    "        transform=transform,\n",
    "        mode=mode,\n",
    "        batch_size=batch_size,\n",
    "        vocab_threshold=vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_word=start_word,\n",
    "        end_word=end_word,\n",
    "        unk_word=unk_word,\n",
    "        annotations_file=annotations_file,\n",
    "        vocab_from_file=vocab_from_file,\n",
    "        img_folder=img_folder,\n",
    "    )\n",
    "\n",
    "    # Configure DataLoader\n",
    "    if mode == \"train\":\n",
    "        indices = dataset.get_train_indices()\n",
    "        sampler = torch.utils.data.SubsetRandomSampler(indices)\n",
    "\n",
    "        # batch_sampler for training\n",
    "        batch_sampler = torch.utils.data.BatchSampler(\n",
    "            sampler=sampler, batch_size=batch_size, drop_last=False\n",
    "        )\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=4,\n",
    "            persistent_workers=True,         )\n",
    "    else:\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=4,\n",
    "            persistent_workers=True, \n",
    "\n",
    "            \n",
    "            \n",
    "        )\n",
    "\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f79d975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.686372Z",
     "iopub.status.busy": "2025-01-23T11:29:32.686179Z",
     "iopub.status.idle": "2025-01-23T11:29:32.699085Z",
     "shell.execute_reply": "2025-01-23T11:29:32.698454Z"
    },
    "papermill": {
     "duration": 0.018987,
     "end_time": "2025-01-23T11:29:32.700185",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.681198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "\n",
    "def val_get_loader(\n",
    "    transform,\n",
    "    mode=\"valid\",\n",
    "    batch_size=1,\n",
    "    vocab_threshold=None,\n",
    "    vocab_file=\"/kaggle/input/vocab/pytorch/default/1/vocab.pkl\",\n",
    "    start_word=\"<start>\",\n",
    "    end_word=\"<end>\",\n",
    "    unk_word=\"<unk>\",\n",
    "    vocab_from_file=True,\n",
    "    num_workers=4,\n",
    "    img_folder=None, \n",
    "    annotations_file=None,  \n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the data loader for the COCO dataset.\n",
    "\n",
    "    Args:\n",
    "        transform: Image transformations (e.g., resizing, normalization).\n",
    "        mode: One of 'train', 'valid', or 'test'.\n",
    "        batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
    "        vocab_threshold: Minimum word count threshold for vocabulary.\n",
    "        vocab_file: Path to save/load the vocabulary.\n",
    "        start_word: Special token for the start of a sentence.\n",
    "        end_word: Special token for the end of a sentence.\n",
    "        unk_word: Special token for unknown words.\n",
    "        vocab_from_file: If True, load vocabulary from file; else, build from scratch.\n",
    "        num_workers: Number of subprocesses to use for data loading.\n",
    "        img_folder: Path to the image folder.\n",
    "        annotations_file: Path to the annotations file.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader for the COCO dataset.\n",
    "    \"\"\"\n",
    "    assert mode in [\"train\", \"valid\", \"test\"], \"mode must be one of 'train', 'valid', or 'test'.\"\n",
    "\n",
    "    if not vocab_from_file:\n",
    "        assert mode == \"train\", \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
    "\n",
    "    if mode == \"valid\" and (not img_folder or not annotations_file):\n",
    "        raise ValueError(\"img_folder and annotations_file must be provided in 'valid' mode.\")\n",
    "\n",
    "    # Initialize COCO dataset\n",
    "    dataset = CoCoDataset(\n",
    "        transform=transform,\n",
    "        mode=mode,\n",
    "        batch_size=batch_size,\n",
    "        vocab_threshold=vocab_threshold,\n",
    "        vocab_file=vocab_file,\n",
    "        start_word=start_word,\n",
    "        end_word=end_word,\n",
    "        unk_word=unk_word,\n",
    "        annotations_file=annotations_file,\n",
    "        vocab_from_file=vocab_from_file,\n",
    "        img_folder=img_folder,\n",
    "    )\n",
    "\n",
    "    # DataLoader for validation or testing mode\n",
    "    data_loader = data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,  # Increase based on your system's cores\n",
    "        pin_memory=True,\n",
    "        shuffle = True,\n",
    "        prefetch_factor=4,\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "class CoCoDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform,\n",
    "        mode,\n",
    "        batch_size,\n",
    "        vocab_threshold,\n",
    "        vocab_file,\n",
    "        start_word,\n",
    "        end_word,\n",
    "        unk_word,\n",
    "        annotations_file,\n",
    "        vocab_from_file,\n",
    "        img_folder,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = Vocabulary(\n",
    "            vocab_threshold,\n",
    "            vocab_file,\n",
    "            start_word,\n",
    "            end_word,\n",
    "            unk_word,\n",
    "            annotations_file,\n",
    "            vocab_from_file,\n",
    "        )\n",
    "        self.img_folder = img_folder\n",
    "        if self.mode == \"train\":\n",
    "            self.coco = COCO(annotations_file)\n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print(\"Obtaining caption lengths...\")\n",
    "            all_tokens = [\n",
    "                nltk.tokenize.word_tokenize(\n",
    "                    str(self.coco.anns[self.ids[index]][\"caption\"]).lower()\n",
    "                )\n",
    "                for index in tqdm(np.arange(len(self.ids)))\n",
    "            ]\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        else:\n",
    "            test_info = json.loads(open(annotations_file).read())\n",
    "            self.paths = [item[\"file_name\"] for item in test_info[\"images\"]]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode == \"train\":\n",
    "            ann_id = self.ids[index]\n",
    "            caption = self.coco.anns[ann_id][\"caption\"]\n",
    "            img_id = self.coco.anns[ann_id][\"image_id\"]\n",
    "            path = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            caption = torch.Tensor(caption).long()\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        elif self.mode == \"valid\":\n",
    "            path = self.paths[index]\n",
    "            image_id = int(path.split(\"/\")[0].split(\".\")[0].split(\"_\")[-1])\n",
    "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            image = self.transform(pil_image)\n",
    "\n",
    "            # return image_id and pre-processed image tensor\n",
    "            return image_id, image\n",
    "\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[index]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            pil_image = Image.open(os.path.join(self.img_folder, path)).convert(\"RGB\")\n",
    "            orig_image = np.array(pil_image)\n",
    "            image = self.transform(pil_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "\n",
    "    def get_train_indices(self):\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where(\n",
    "            [\n",
    "                self.caption_lengths[i] == sel_length\n",
    "                for i in np.arange(len(self.caption_lengths))\n",
    "            ]\n",
    "        )[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602191e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.709873Z",
     "iopub.status.busy": "2025-01-23T11:29:32.709668Z",
     "iopub.status.idle": "2025-01-23T11:29:32.717938Z",
     "shell.execute_reply": "2025-01-23T11:29:32.717384Z"
    },
    "papermill": {
     "duration": 0.014543,
     "end_time": "2025-01-23T11:29:32.719131",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.704588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------- Encoder ------------\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "# --------- Decoder ----------\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_size: final embedding size of the CNN encoder\n",
    "            hidden_size: hidden size of the LSTM\n",
    "            vocab_size: size of the vocabulary\n",
    "            num_layers: number of layers of the LSTM\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Assigning hidden dimension\n",
    "        self.hidden_dim = hidden_size\n",
    "        # Map each word index to a dense word embedding tensor of embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # Creating LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Initializing linear to apply at last of RNN layer for further prediction\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # Initializing values for hidden and cell state\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: features tensor. shape is (bs, embed_size)\n",
    "            captions: captions tensor. shape is (bs, cap_length)\n",
    "        Returns:\n",
    "            outputs: scores of the linear layer\n",
    "\n",
    "        \"\"\"\n",
    "        # remove <end> token from captions and embed captions\n",
    "        cap_embedding = self.embed(\n",
    "            captions[:, :-1]\n",
    "        )  # (bs, cap_length) -> (bs, cap_length-1, embed_size)\n",
    "\n",
    "        # concatenate the images features to the first of caption embeddings.\n",
    "        # [bs, embed_size] => [bs, 1, embed_size] concat [bs, cap_length-1, embed_size]\n",
    "        # => [bs, cap_length, embed_size] add encoded image (features) as t=0\n",
    "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
    "\n",
    "        #  getting output i.e. score and hidden layer.\n",
    "        # first value: all the hidden states throughout the sequence. second value: the most recent hidden state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeddings\n",
    "        )  # (bs, cap_length, hidden_size), (1, bs, hidden_size)\n",
    "        outputs = self.linear(lstm_out)  # (bs, cap_length, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=50):\n",
    "        \"\"\"\n",
    "        accepts pre-processed image tensor (inputs) and returns predicted\n",
    "        sentence (list of tensor ids of length max_len)\n",
    "        Args:\n",
    "            inputs: shape is (1, 1, embed_size)\n",
    "            states: initial hidden state of the LSTM\n",
    "            max_len: maximum length of the predicted sentence\n",
    "\n",
    "        Returns:\n",
    "            res: list of predicted words indices\n",
    "        \"\"\"\n",
    "        res = []\n",
    "\n",
    "        for i in range(max_len):\n",
    "            lstm_out, states = self.lstm(\n",
    "                inputs, states\n",
    "            )  # lstm_out: (1, 1, hidden_size)\n",
    "            outputs = self.linear(lstm_out.squeeze(dim=1))  # outputs: (1, vocab_size)\n",
    "            _, predicted_idx = outputs.max(dim=1)  # predicted: (1, 1)\n",
    "            res.append(predicted_idx.item())\n",
    "            # if the predicted idx is the stop index, the loop stops\n",
    "            if predicted_idx == 1:\n",
    "                break\n",
    "            inputs = self.embed(predicted_idx)  # inputs: (1, embed_size)\n",
    "            # prepare input for next iteration\n",
    "            inputs = inputs.unsqueeze(1)  # inputs: (1, 1, embed_size)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c62d1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.728756Z",
     "iopub.status.busy": "2025-01-23T11:29:32.728565Z",
     "iopub.status.idle": "2025-01-23T11:29:32.886051Z",
     "shell.execute_reply": "2025-01-23T11:29:32.885402Z"
    },
    "papermill": {
     "duration": 0.163662,
     "end_time": "2025-01-23T11:29:32.887376",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.723714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.utils.data as data\n",
    "from collections import defaultdict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969834a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.897346Z",
     "iopub.status.busy": "2025-01-23T11:29:32.897118Z",
     "iopub.status.idle": "2025-01-23T11:29:32.933755Z",
     "shell.execute_reply": "2025-01-23T11:29:32.932925Z"
    },
    "papermill": {
     "duration": 0.042867,
     "end_time": "2025-01-23T11:29:32.934935",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.892068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Base dataset paths\n",
    "base_path_dataset = Path('/kaggle/input/mscoco')\n",
    "base_path_annotation = Path('/kaggle/input/coco-image-caption/annotations_trainval2014')\n",
    "\n",
    "# Annotation files\n",
    "train_annot_file = base_path_annotation / 'annotations' / 'captions_train2014.json'\n",
    "val_annot_file = base_path_annotation / 'annotations' / 'captions_val2014.json'\n",
    "\n",
    "# Image folders\n",
    "train_images_folder = base_path_dataset / 'train2014' / 'train2014'\n",
    "val_images_folder = base_path_dataset / 'val2014' / 'val2014'\n",
    "test_images_folder = base_path_dataset / 'test2014' / 'test2014'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64ebcd48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.944580Z",
     "iopub.status.busy": "2025-01-23T11:29:32.944359Z",
     "iopub.status.idle": "2025-01-23T11:29:32.979503Z",
     "shell.execute_reply": "2025-01-23T11:29:32.978976Z"
    },
    "papermill": {
     "duration": 0.041212,
     "end_time": "2025-01-23T11:29:32.980592",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.939380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128  \n",
    "vocab_threshold = 5  # minimum word count threshold\n",
    "vocab_from_file = True  # if True, load existing vocab file\n",
    "embed_size = 128  # dimensionality of image and word embeddings\n",
    "hidden_size = 256  # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 5  # number of training epochs\n",
    "save_every = 1  # determines frequency of saving model weights\n",
    "print_every = 20  # determines window for printing average loss\n",
    "log_file = \"/kaggle/working/training_log.txt\"  # path to save training log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "419b34b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:32.990105Z",
     "iopub.status.busy": "2025-01-23T11:29:32.989894Z",
     "iopub.status.idle": "2025-01-23T11:29:33.024407Z",
     "shell.execute_reply": "2025-01-23T11:29:33.023859Z"
    },
    "papermill": {
     "duration": 0.040522,
     "end_time": "2025-01-23T11:29:33.025582",
     "exception": false,
     "start_time": "2025-01-23T11:29:32.985060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image transformations for training\n",
    "transform_train = transforms.Compose([\n",
    "    # Smaller edge of image resized to 256\n",
    "    transforms.Resize(256),\n",
    "    # Get 224x224 crop from random location\n",
    "    transforms.RandomCrop(224),\n",
    "    # Horizontally flip image with probability=0.5\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Convert the PIL Image to a tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize image for pre-trained model\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66f641bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:29:33.035200Z",
     "iopub.status.busy": "2025-01-23T11:29:33.034931Z",
     "iopub.status.idle": "2025-01-23T11:30:52.814843Z",
     "shell.execute_reply": "2025-01-23T11:30:52.814021Z"
    },
    "papermill": {
     "duration": 79.786278,
     "end_time": "2025-01-23T11:30:52.816329",
     "exception": false,
     "start_time": "2025-01-23T11:29:33.030051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.30s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenizing captions...\n",
      "[100000/414113] Tokenizing captions...\n",
      "[200000/414113] Tokenizing captions...\n",
      "[300000/414113] Tokenizing captions...\n",
      "[400000/414113] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98ba0b9fdf647fc83bf2a1670f4fa04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = get_loader(\n",
    "    transform=transform_train,\n",
    "    mode=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    vocab_threshold=vocab_threshold,\n",
    "    vocab_file=\"/kaggle/working/vocab.pkl\",\n",
    "    start_word=\"<start>\",\n",
    "    end_word=\"<end>\",\n",
    "    unk_word=\"<unk>\",\n",
    "    vocab_from_file=False,\n",
    "    num_workers=4,\n",
    "    img_folder=str(train_images_folder),\n",
    "    annotations_file=str(train_annot_file),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4964e61f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:30:52.827985Z",
     "iopub.status.busy": "2025-01-23T11:30:52.827698Z",
     "iopub.status.idle": "2025-01-23T11:30:52.874906Z",
     "shell.execute_reply": "2025-01-23T11:30:52.873939Z"
    },
    "papermill": {
     "duration": 0.054246,
     "end_time": "2025-01-23T11:30:52.876226",
     "exception": false,
     "start_time": "2025-01-23T11:30:52.821980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------- Encoder ------------\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # disable learning for parameters\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "# --------- Decoder ----------\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_size: final embedding size of the CNN encoder\n",
    "            hidden_size: hidden size of the LSTM\n",
    "            vocab_size: size of the vocabulary\n",
    "            num_layers: number of layers of the LSTM\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # Assigning hidden dimension\n",
    "        self.hidden_dim = hidden_size\n",
    "        # Map each word index to a dense word embedding tensor of embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # Creating LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Initializing linear to apply at last of RNN layer for further prediction\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # Initializing values for hidden and cell state\n",
    "        self.hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: features tensor. shape is (bs, embed_size)\n",
    "            captions: captions tensor. shape is (bs, cap_length)\n",
    "        Returns:\n",
    "            outputs: scores of the linear layer\n",
    "\n",
    "        \"\"\"\n",
    "        # remove <end> token from captions and embed captions\n",
    "        cap_embedding = self.embed(\n",
    "            captions[:, :-1]\n",
    "        )  # (bs, cap_length) -> (bs, cap_length-1, embed_size)\n",
    "\n",
    "        embeddings = torch.cat((features.unsqueeze(dim=1), cap_embedding), dim=1)\n",
    "\n",
    "        #  getting output i.e. score and hidden layer.\n",
    "        # first value: all the hidden states throughout the sequence. second value: the most recent hidden state\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeddings\n",
    "        )  # (bs, cap_length, hidden_size), (1, bs, hidden_size)\n",
    "        outputs = self.linear(lstm_out)  # (bs, cap_length, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=50):\n",
    "        \"\"\"\n",
    "        accepts pre-processed image tensor (inputs) and returns predicted\n",
    "        sentence (list of tensor ids of length max_len)\n",
    "        Args:\n",
    "            inputs: shape is (1, 1, embed_size)\n",
    "            states: initial hidden state of the LSTM\n",
    "            max_len: maximum length of the predicted sentence\n",
    "\n",
    "        Returns:\n",
    "            res: list of predicted words indices\n",
    "        \"\"\"\n",
    "        res = []\n",
    "\n",
    "        # Now we feed the LSTM output and hidden states back into itself to get the caption\n",
    "        for i in range(max_len):\n",
    "            lstm_out, states = self.lstm(\n",
    "                inputs, states\n",
    "            )  # lstm_out: (1, 1, hidden_size)\n",
    "            outputs = self.linear(lstm_out.squeeze(dim=1))  # outputs: (1, vocab_size)\n",
    "            _, predicted_idx = outputs.max(dim=1)  # predicted: (1, 1)\n",
    "            res.append(predicted_idx.item())\n",
    "            # if the predicted idx is the stop index, the loop stops\n",
    "            if predicted_idx == 1:\n",
    "                break\n",
    "            inputs = self.embed(predicted_idx)  # inputs: (1, embed_size)\n",
    "            # prepare input for next iteration\n",
    "            inputs = inputs.unsqueeze(1)  # inputs: (1, 1, embed_size)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "873beb02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:30:52.887485Z",
     "iopub.status.busy": "2025-01-23T11:30:52.887254Z",
     "iopub.status.idle": "2025-01-23T11:30:54.357972Z",
     "shell.execute_reply": "2025-01-23T11:30:54.357000Z"
    },
    "papermill": {
     "duration": 1.47831,
     "end_time": "2025-01-23T11:30:54.359857",
     "exception": false,
     "start_time": "2025-01-23T11:30:52.881547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is :  8855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:00<00:00, 204MB/s]\n"
     ]
    }
   ],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "print(\"vocab size is : \",vocab_size)\n",
    "\n",
    "# Initializing the encoder and decoder\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Defining the loss function\n",
    "criterion = (\n",
    "    nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Specifying the learnable parameters of the mode\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Defining the optimize\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoc\n",
    "total_step = math.ceil(len(data_loader.dataset) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dacab4fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:30:54.373097Z",
     "iopub.status.busy": "2025-01-23T11:30:54.372805Z",
     "iopub.status.idle": "2025-01-23T11:30:54.411997Z",
     "shell.execute_reply": "2025-01-23T11:30:54.410997Z"
    },
    "papermill": {
     "duration": 0.046852,
     "end_time": "2025-01-23T11:30:54.413188",
     "exception": false,
     "start_time": "2025-01-23T11:30:54.366336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3236\n"
     ]
    }
   ],
   "source": [
    "print(total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b94db10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-23T11:30:54.425614Z",
     "iopub.status.busy": "2025-01-23T11:30:54.425401Z",
     "iopub.status.idle": "2025-01-23T18:05:43.831933Z",
     "shell.execute_reply": "2025-01-23T18:05:43.830512Z"
    },
    "papermill": {
     "duration": 23689.414716,
     "end_time": "2025-01-23T18:05:43.833632",
     "exception": false,
     "start_time": "2025-01-23T11:30:54.418916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-1adaa1eb62c6>:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # Updated per the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/3236 [00:00<?, ?step/s]<ipython-input-16-1adaa1eb62c6>:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1: 100%|██████████| 3236/3236 [1:26:28<00:00,  1.60s/step, Loss=2.33, Perplexity=10.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3236/3236 [1:16:34<00:00,  1.42s/step, Loss=2.01, Perplexity=7.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3236/3236 [1:16:49<00:00,  1.42s/step, Loss=2.21, Perplexity=9.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3236/3236 [1:16:11<00:00,  1.41s/step, Loss=2.16, Perplexity=8.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 3236/3236 [1:18:43<00:00,  1.46s/step, Loss=2.21, Perplexity=9.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Best model saved from epoch 5 with loss 2.0569.\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.makedirs(\"/kaggle/working/models\", exist_ok=True)\n",
    "\n",
    "# Initialize GradScaler for mixed precision training\n",
    "scaler = GradScaler()  \n",
    "\n",
    "best_loss = float(\"inf\") \n",
    "best_epoch = 0 \n",
    "save_every = 1  # Save model every epochs\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    epoch_loss = 0  \n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "    with tqdm(total=total_step, desc=f\"Epoch {epoch}\", unit=\"step\") as pbar:\n",
    "        for i_step in range(1, total_step + 1):\n",
    "\n",
    "            # Randomly sample a caption length and indices with that length\n",
    "            indices = data_loader.dataset.get_train_indices()\n",
    "            new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "            data_loader.batch_sampler.sampler = new_sampler\n",
    "\n",
    "            # Obtain batch data\n",
    "            images, captions = next(iter(data_loader))\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mixed precision training\n",
    "            with autocast():  \n",
    "                features = encoder(images)\n",
    "                outputs = decoder(features, captions)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # Scale loss and backpropagate\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"Loss\": loss.item(), \"Perplexity\": np.exp(loss.item())})\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    epoch_loss /= total_step\n",
    "\n",
    "    # Save the best model if the current epoch's loss is lower\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(encoder, os.path.join(\"/kaggle/working/models\", \"best_encoder.pkl\"))\n",
    "        torch.save(decoder, os.path.join(\"/kaggle/working/models\", \"best_decoder.pkl\"))\n",
    "\n",
    "    # Save model weights every 'save_every' epochs\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(encoder.state_dict(), os.path.join(\"/kaggle/working/models\", f\"encoder-{epoch}.pkl\"))\n",
    "        torch.save(decoder.state_dict(), os.path.join(\"/kaggle/working/models\", f\"decoder-{epoch}.pkl\"))\n",
    "\n",
    "print(f\"Training complete. Best model saved from epoch {best_epoch} with loss {best_loss:.4f}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3445072,
     "sourceId": 6019472,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6420335,
     "sourceId": 10365806,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 225344,
     "modelInstanceId": 203621,
     "sourceId": 238425,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23792.155336,
   "end_time": "2025-01-23T18:05:52.354397",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-23T11:29:20.199061",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3ed1392536c04e3d821517feea600802": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d4e9d230362c4ecea42a9fb4dc7f74b7",
       "placeholder": "​",
       "style": "IPY_MODEL_f5930d00bcdf4481842bb4ae90e0fb4d",
       "tabbable": null,
       "tooltip": null,
       "value": " 414113/414113 [00:39&lt;00:00, 9031.84it/s]"
      }
     },
     "4578dd959b9d473581c1b633e237e5ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7913578e97ff491f808ef9ec6affe5fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4578dd959b9d473581c1b633e237e5ec",
       "max": 414113,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_80d663470aa841358c3ed59b54b88b4d",
       "tabbable": null,
       "tooltip": null,
       "value": 414113
      }
     },
     "79442e25265f4c6db218573200ffe363": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ea66cbb5f4ad4d24b6a8640e76ab4331",
       "placeholder": "​",
       "style": "IPY_MODEL_f9d956202f304fb3b0310897b6a7d2a3",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "80d663470aa841358c3ed59b54b88b4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d4e9d230362c4ecea42a9fb4dc7f74b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d98ba0b9fdf647fc83bf2a1670f4fa04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_79442e25265f4c6db218573200ffe363",
        "IPY_MODEL_7913578e97ff491f808ef9ec6affe5fb",
        "IPY_MODEL_3ed1392536c04e3d821517feea600802"
       ],
       "layout": "IPY_MODEL_f36059b0fac24c9fa6bbc8295bf39575",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ea66cbb5f4ad4d24b6a8640e76ab4331": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f36059b0fac24c9fa6bbc8295bf39575": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5930d00bcdf4481842bb4ae90e0fb4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f9d956202f304fb3b0310897b6a7d2a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
